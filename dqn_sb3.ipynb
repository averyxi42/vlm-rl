{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgDiMlHXXN7V"
      },
      "source": [
        "# DQN and Double DQN with Stable-Baselines3\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "Double Q-Learning: https://paperswithcode.com/method/double-q-learning\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we will study DQN using Stable-Baselines3 and then see how to reduce value overestimation with double DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmMaKjrX6MC"
      },
      "source": [
        "## Installation\n",
        "\n",
        "We will install master version of SB3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MzmFLcdtGh7D"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r7yLmacAXJ0F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 167 not upgraded.\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a4 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.7.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.6)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.9.0)\n",
            "Requirement already satisfied: cloudpickle in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.1)\n",
            "Requirement already satisfied: pandas in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.3.3)\n",
            "Requirement already satisfied: matplotlib in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
            "Requirement already satisfied: filelock in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.5.0)\n",
            "Requirement already satisfied: opencv-python in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.20.0)\n",
            "Requirement already satisfied: psutil in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.67.1)\n",
            "Requirement already satisfied: rich in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (14.2.0)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (0.11.2)\n",
            "Requirement already satisfied: pillow in /Projects/envs/sd3/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a4) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.3.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.9)\n",
            "Requirement already satisfied: packaging in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (6.33.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (80.9.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=3 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Projects/envs/sd3/lib/python3.10/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Projects/envs/sd3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Projects/envs/sd3/lib/python3.10/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Projects/envs/sd3/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Projects/envs/sd3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwC8l-17YseR"
      },
      "source": [
        "Import DQN and evaluation helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VYbeqK0tYenp"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xQwxJDgptxH"
      },
      "source": [
        "## The Mountain Car Problem\n",
        "\n",
        "In this environment, the agent must drive an underpowered car up a steep mountain road. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n",
        "\n",
        "Source: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/mountain_car.py\n",
        "\n",
        "![mountaincar.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "    Observation:\n",
        "        Type: Box(2)\n",
        "        Num    Observation               Min            Max\n",
        "        0      Car Position              -1.2           0.6\n",
        "        1      Car Velocity              -0.07          0.07\n",
        "    Actions:\n",
        "        Type: Discrete(3)\n",
        "        Num    Action\n",
        "        0      Accelerate to the Left\n",
        "        1      Don't accelerate\n",
        "        2      Accelerate to the Right\n",
        "        Note: This does not affect the amount of velocity affected by the\n",
        "        gravitational pull acting on the car.\n",
        "    Reward:\n",
        "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
        "         on top of the mountain.\n",
        "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
        "    Starting State:\n",
        "         The position of the car is assigned a uniform random value in\n",
        "         [-0.6 , -0.4].\n",
        "         The starting velocity of the car is always assigned to 0.\n",
        "    Episode Termination:\n",
        "         The car position is more than 0.5\n",
        "         Episode length is greater than 200\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og59Z62aZJna"
      },
      "source": [
        "Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tVY05GIhZEMM"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEF4P0DAZMAN"
      },
      "source": [
        "Create the model with tuned hyperparameters from the RL Zoo\n",
        "\n",
        "```yaml\n",
        "MountainCar-v0:\n",
        "  n_timesteps: !!float 1.2e5\n",
        "  policy: 'MlpPolicy'\n",
        "  learning_rate: !!float 4e-3\n",
        "  batch_size: 128\n",
        "  buffer_size: 10000\n",
        "  learning_starts: 1000\n",
        "  gamma: 0.98\n",
        "  target_update_interval: 600\n",
        "  train_freq: 16\n",
        "  gradient_steps: 8\n",
        "  exploration_fraction: 0.2\n",
        "  exploration_final_eps: 0.07\n",
        "  policy_kwargs: \"dict(net_arch=[256, 256])\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fbEcqWhqgDmH"
      },
      "outputs": [],
      "source": [
        "tensorboard_log = \"data/tb/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4-1scts3Y1c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "dqn_model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    verbose=1,\n",
        "    train_freq=16,\n",
        "    gradient_steps=8,\n",
        "    gamma=0.99,\n",
        "    exploration_fraction=0.2,\n",
        "    exploration_final_eps=0.07,\n",
        "    target_update_interval=600,\n",
        "    learning_starts=1000,\n",
        "    buffer_size=10000,\n",
        "    batch_size=128,\n",
        "    learning_rate=4e-3,\n",
        "    policy_kwargs=dict(net_arch=[256, 256]),\n",
        "    tensorboard_log=tensorboard_log,\n",
        "    seed=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNoFwsCPZQuz"
      },
      "source": [
        "Evaluate the agent before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qn0C7RHyZTHA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_reward:-200.00 +/- 0.00\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(\n",
        "    dqn_model,\n",
        "    dqn_model.get_env(),\n",
        "    deterministic=True,\n",
        "    n_eval_episodes=20,\n",
        ")\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kM0m1NyAgVhR"
      },
      "outputs": [],
      "source": [
        "# Optional: Monitor training in tensorboard\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir $tensorboard_log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUoPUfQ4ZexC"
      },
      "source": [
        "We will first train the agent until convergence and then analyse the learned q-value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9wKP0gKjZgWZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to data/tb/DQN_1\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.922    |\n",
            "| time/               |          |\n",
            "|    episodes         | 10       |\n",
            "|    fps              | 1572     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 2000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 1.97e-05 |\n",
            "|    n_updates        | 496      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.845    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 1235     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 4000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 9.68e-07 |\n",
            "|    n_updates        | 1496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.767    |\n",
            "| time/               |          |\n",
            "|    episodes         | 30       |\n",
            "|    fps              | 1138     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 6000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 8.07e-06 |\n",
            "|    n_updates        | 2496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.69     |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 1088     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 8000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 1.41e-05 |\n",
            "|    n_updates        | 3496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.612    |\n",
            "| time/               |          |\n",
            "|    episodes         | 50       |\n",
            "|    fps              | 1057     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 10000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 3.17e-06 |\n",
            "|    n_updates        | 4496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.535    |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 1032     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 12000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 4.93e-06 |\n",
            "|    n_updates        | 5496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.457    |\n",
            "| time/               |          |\n",
            "|    episodes         | 70       |\n",
            "|    fps              | 1011     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 14000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 1.84e-05 |\n",
            "|    n_updates        | 6496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.38     |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 992      |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total_timesteps  | 16000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 8.12e-06 |\n",
            "|    n_updates        | 7496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.302    |\n",
            "| time/               |          |\n",
            "|    episodes         | 90       |\n",
            "|    fps              | 977      |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 18000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 9.54e-06 |\n",
            "|    n_updates        | 8496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.225    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 963      |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 20000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 8.48e-05 |\n",
            "|    n_updates        | 9496     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.147    |\n",
            "| time/               |          |\n",
            "|    episodes         | 110      |\n",
            "|    fps              | 949      |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total_timesteps  | 22000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 5.83e-06 |\n",
            "|    n_updates        | 10496    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.0714   |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 933      |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 23965    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0283   |\n",
            "|    n_updates        | 11480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 130      |\n",
            "|    fps              | 922      |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 25965    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.00107  |\n",
            "|    n_updates        | 12480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 199      |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 913      |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 27901    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.000702 |\n",
            "|    n_updates        | 13448    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 199      |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 150      |\n",
            "|    fps              | 905      |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 29880    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.000741 |\n",
            "|    n_updates        | 14440    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 198      |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 899      |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 31811    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 15408    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 197      |\n",
            "|    ep_rew_mean      | -197     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 170      |\n",
            "|    fps              | 894      |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 33717    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0107   |\n",
            "|    n_updates        | 16360    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 197      |\n",
            "|    ep_rew_mean      | -197     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 888      |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 35692    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0106   |\n",
            "|    n_updates        | 17344    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 196      |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 190      |\n",
            "|    fps              | 884      |\n",
            "|    time_elapsed     | 42       |\n",
            "|    total_timesteps  | 37619    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.00554  |\n",
            "|    n_updates        | 18312    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 195      |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 39538    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.00388  |\n",
            "|    n_updates        | 19272    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 192      |\n",
            "|    ep_rew_mean      | -192     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 210      |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 41240    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0226   |\n",
            "|    n_updates        | 20120    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 191      |\n",
            "|    ep_rew_mean      | -191     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 43066    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0718   |\n",
            "|    n_updates        | 21032    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 189      |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 230      |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 44843    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.00951  |\n",
            "|    n_updates        | 21920    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 186      |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 46528    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.078    |\n",
            "|    n_updates        | 22760    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 183      |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 250      |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 48175    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.048    |\n",
            "|    n_updates        | 23584    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 183      |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 50107    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0533   |\n",
            "|    n_updates        | 24552    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 181      |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 270      |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 59       |\n",
            "|    total_timesteps  | 51862    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0838   |\n",
            "|    n_updates        | 25432    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 180      |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 862      |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total_timesteps  | 53692    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0731   |\n",
            "|    n_updates        | 26344    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 180      |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 290      |\n",
            "|    fps              | 860      |\n",
            "|    time_elapsed     | 64       |\n",
            "|    total_timesteps  | 55624    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 27312    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 178      |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 859      |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 57388    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 28192    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 179      |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 310      |\n",
            "|    fps              | 858      |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total_timesteps  | 59141    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0669   |\n",
            "|    n_updates        | 29072    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 177      |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 320      |\n",
            "|    fps              | 856      |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 60745    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 29872    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 176      |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 330      |\n",
            "|    fps              | 855      |\n",
            "|    time_elapsed     | 72       |\n",
            "|    total_timesteps  | 62448    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0944   |\n",
            "|    n_updates        | 30720    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 175      |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 854      |\n",
            "|    time_elapsed     | 74       |\n",
            "|    total_timesteps  | 63991    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 31496    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 172      |\n",
            "|    ep_rew_mean      | -172     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 350      |\n",
            "|    fps              | 853      |\n",
            "|    time_elapsed     | 76       |\n",
            "|    total_timesteps  | 65392    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 32192    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 169      |\n",
            "|    ep_rew_mean      | -169     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 852      |\n",
            "|    time_elapsed     | 78       |\n",
            "|    total_timesteps  | 66992    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.184    |\n",
            "|    n_updates        | 32992    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 166      |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 370      |\n",
            "|    fps              | 851      |\n",
            "|    time_elapsed     | 80       |\n",
            "|    total_timesteps  | 68473    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.133    |\n",
            "|    n_updates        | 33736    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 162      |\n",
            "|    ep_rew_mean      | -162     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 850      |\n",
            "|    time_elapsed     | 82       |\n",
            "|    total_timesteps  | 69867    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 34432    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 157      |\n",
            "|    ep_rew_mean      | -157     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 390      |\n",
            "|    fps              | 850      |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 71311    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.184    |\n",
            "|    n_updates        | 35152    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 153      |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 849      |\n",
            "|    time_elapsed     | 85       |\n",
            "|    total_timesteps  | 72719    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.241    |\n",
            "|    n_updates        | 35856    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 150      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 410      |\n",
            "|    fps              | 848      |\n",
            "|    time_elapsed     | 87       |\n",
            "|    total_timesteps  | 74139    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 36568    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -147     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 847      |\n",
            "|    time_elapsed     | 88       |\n",
            "|    total_timesteps  | 75438    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 37216    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 430      |\n",
            "|    fps              | 847      |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total_timesteps  | 76657    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.178    |\n",
            "|    n_updates        | 37832    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 140      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 440      |\n",
            "|    fps              | 845      |\n",
            "|    time_elapsed     | 92       |\n",
            "|    total_timesteps  | 77946    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.171    |\n",
            "|    n_updates        | 38472    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 139      |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 450      |\n",
            "|    fps              | 844      |\n",
            "|    time_elapsed     | 93       |\n",
            "|    total_timesteps  | 79272    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.204    |\n",
            "|    n_updates        | 39136    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 136      |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 460      |\n",
            "|    fps              | 843      |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 80602    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.154    |\n",
            "|    n_updates        | 39800    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 134      |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 470      |\n",
            "|    fps              | 843      |\n",
            "|    time_elapsed     | 97       |\n",
            "|    total_timesteps  | 81919    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.217    |\n",
            "|    n_updates        | 40456    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 134      |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 480      |\n",
            "|    fps              | 843      |\n",
            "|    time_elapsed     | 98       |\n",
            "|    total_timesteps  | 83243    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.263    |\n",
            "|    n_updates        | 41120    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 132      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 490      |\n",
            "|    fps              | 843      |\n",
            "|    time_elapsed     | 100      |\n",
            "|    total_timesteps  | 84538    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.165    |\n",
            "|    n_updates        | 41768    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 132      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 500      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 101      |\n",
            "|    total_timesteps  | 85905    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.243    |\n",
            "|    n_updates        | 42456    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 131      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 510      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 103      |\n",
            "|    total_timesteps  | 87190    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.203    |\n",
            "|    n_updates        | 43096    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 520      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 104      |\n",
            "|    total_timesteps  | 88347    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.2      |\n",
            "|    n_updates        | 43672    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 530      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 106      |\n",
            "|    total_timesteps  | 89583    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.203    |\n",
            "|    n_updates        | 44288    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 540      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 107      |\n",
            "|    total_timesteps  | 90833    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.238    |\n",
            "|    n_updates        | 44920    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 550      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 109      |\n",
            "|    total_timesteps  | 91917    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.195    |\n",
            "|    n_updates        | 45456    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 124      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 560      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 110      |\n",
            "|    total_timesteps  | 92998    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.161    |\n",
            "|    n_updates        | 46000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 122      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 570      |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 111      |\n",
            "|    total_timesteps  | 94150    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 46576    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 120      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 580      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 113      |\n",
            "|    total_timesteps  | 95290    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 47144    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 119      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 590      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 114      |\n",
            "|    total_timesteps  | 96402    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0994   |\n",
            "|    n_updates        | 47704    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 117      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 600      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 115      |\n",
            "|    total_timesteps  | 97604    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 48304    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 610      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 98714    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 48856    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 620      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 118      |\n",
            "|    total_timesteps  | 99830    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 49416    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 630      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 120      |\n",
            "|    total_timesteps  | 101001   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0937   |\n",
            "|    n_updates        | 50000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 640      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 121      |\n",
            "|    total_timesteps  | 102221   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 50608    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 650      |\n",
            "|    fps              | 840      |\n",
            "|    time_elapsed     | 122      |\n",
            "|    total_timesteps  | 103375   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0956   |\n",
            "|    n_updates        | 51184    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 660      |\n",
            "|    fps              | 839      |\n",
            "|    time_elapsed     | 124      |\n",
            "|    total_timesteps  | 104526   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.11     |\n",
            "|    n_updates        | 51760    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 670      |\n",
            "|    fps              | 839      |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total_timesteps  | 105558   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.153    |\n",
            "|    n_updates        | 52280    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 680      |\n",
            "|    fps              | 838      |\n",
            "|    time_elapsed     | 127      |\n",
            "|    total_timesteps  | 106798   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 52896    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 690      |\n",
            "|    fps              | 838      |\n",
            "|    time_elapsed     | 128      |\n",
            "|    total_timesteps  | 107964   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.0607   |\n",
            "|    n_updates        | 53480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 700      |\n",
            "|    fps              | 838      |\n",
            "|    time_elapsed     | 130      |\n",
            "|    total_timesteps  | 109127   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 54064    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 710      |\n",
            "|    fps              | 838      |\n",
            "|    time_elapsed     | 131      |\n",
            "|    total_timesteps  | 110219   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 54608    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 837      |\n",
            "|    time_elapsed     | 133      |\n",
            "|    total_timesteps  | 111464   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 55232    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 117      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 730      |\n",
            "|    fps              | 837      |\n",
            "|    time_elapsed     | 134      |\n",
            "|    total_timesteps  | 112679   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.202    |\n",
            "|    n_updates        | 55840    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 740      |\n",
            "|    fps              | 837      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 113788   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 56392    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 750      |\n",
            "|    fps              | 836      |\n",
            "|    time_elapsed     | 137      |\n",
            "|    total_timesteps  | 114918   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.182    |\n",
            "|    n_updates        | 56960    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 760      |\n",
            "|    fps              | 836      |\n",
            "|    time_elapsed     | 138      |\n",
            "|    total_timesteps  | 116078   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 57536    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 770      |\n",
            "|    fps              | 836      |\n",
            "|    time_elapsed     | 140      |\n",
            "|    total_timesteps  | 117148   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 58072    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 780      |\n",
            "|    fps              | 835      |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 118276   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.155    |\n",
            "|    n_updates        | 58640    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.07     |\n",
            "| time/               |          |\n",
            "|    episodes         | 790      |\n",
            "|    fps              | 835      |\n",
            "|    time_elapsed     | 143      |\n",
            "|    total_timesteps  | 119535   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.004    |\n",
            "|    loss             | 0.228    |\n",
            "|    n_updates        | 59264    |\n",
            "----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7fb97095ea70>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dqn_model.learn(int(1.2e5), log_interval=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI8xh463Zi7M"
      },
      "source": [
        "Evaluate after training, the mean episodic reward should have improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z952YogYZ8yD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_reward:-104.25 +/- 11.27\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "def record_video(\n",
        "    env_id,\n",
        "    model,\n",
        "    video_length=500,\n",
        "    prefix=\"\",\n",
        "    video_folder=\"videos/\",\n",
        "):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs, deterministic=False)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFiOqKE3aDzI"
      },
      "source": [
        "## Visualize trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MvVGX13xaGbf"
      },
      "outputs": [
        {
          "ename": "DependencyNotInstalled",
          "evalue": "MoviePy is not installed, run `pip install 'gymnasium[other]'`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/conda/envs/sd3/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:77\u001b[0m, in \u001b[0;36mVecVideoRecorder.__init__\u001b[0;34m(self, venv, video_folder, record_video_trigger, video_length, name_prefix)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMountainCar-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdqn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqn-mountaincar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     18\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: gym\u001b[38;5;241m.\u001b[39mmake(env_id, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Start the video at step=0 and record 500 steps\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m \u001b[43mVecVideoRecorder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_video_trigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m obs \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_length):\n",
            "File \u001b[0;32m~/conda/envs/sd3/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:79\u001b[0m, in \u001b[0;36mVecVideoRecorder.__init__\u001b[0;34m(self, venv, video_folder, record_video_trigger, video_length, name_prefix)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoviePy is not installed, run `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgymnasium[other]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: MoviePy is not installed, run `pip install 'gymnasium[other]'`"
          ]
        }
      ],
      "source": [
        "record_video(\"MountainCar-v0\", dqn_model, video_length=500, prefix=\"dqn-mountaincar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHaYvk8uaK70"
      },
      "outputs": [],
      "source": [
        "show_videos(\"videos\", prefix=\"dqn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY00dtf4aRLQ"
      },
      "source": [
        "## Visualize Q-values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP3YH2x7rieM"
      },
      "source": [
        "### Exercise (5 minutes): Retrieve q-values\n",
        "\n",
        "The function will be used to retrieve the learned q-values for a given state (`observation` in the code).\n",
        "\n",
        "The q-network from SB3 DQN can be accessed via `model.q_net` and is a PyTorch module (you can therefore call `.forward()` on it).\n",
        "\n",
        "You need to convert the observation to a PyTorch tensor and then convert the resulting q-values to numpy array.\n",
        "\n",
        "Note: It is recommended to use `with th.no_grad():` context to save computation and memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiCp8OpCbKZW"
      },
      "outputs": [],
      "source": [
        "def get_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieve Q-values for a given observation.\n",
        "\n",
        "    :param model: a DQN model\n",
        "    :param obs: a single observation\n",
        "    :return: the associated q-values for the given observation\n",
        "    \"\"\"\n",
        "    assert model.get_env().observation_space.contains(obs), f\"Invalid observation of shape {obs.shape}: {obs}\"\n",
        "    ### YOUR CODE HERE\n",
        "    # Retrieve q-values for the given observation and convert them to numpy\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    assert isinstance(q_values, np.ndarray), \"The returned q_values is not a numpy array\"\n",
        "    assert q_values.shape == (3,), f\"Wrong shape: (3,) was expected but got {q_values.shape}\"\n",
        "\n",
        "    return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcudYVkdbB0e"
      },
      "source": [
        "### Q-values for the initial state\n",
        "\n",
        "Let's reset the environment to start a new episode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izFixcWgaVe3"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ouWnFaut0KB"
      },
      "source": [
        "we plot the rendered environment to visualize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF1L_1Gfal-g"
      },
      "outputs": [],
      "source": [
        "plt.axis('off')\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKIGyJ-bEQO"
      },
      "source": [
        "### Exercise (5 minutes): predict taken action according to q-values\n",
        "\n",
        "Using the `get_q_values()` function, retrieve the q-values for the initial observation, print them for each action (\"left\", \"nothing\", \"right\") and print the action that the greedy (deterministic) policy would follow (i.e., the action with the highest q-value for that state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPegCYbSuY-f"
      },
      "outputs": [],
      "source": [
        "action_str = [\"Left\", \"Nothing\", \"Right\"]  # action=0 -> go left, action=1 -> do nothing, action=2 -> go right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv90MTHvaqbV"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Retrieve q-values for the initial state\n",
        "# You should use `get_q_values()`\n",
        "\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(f\"Q-value of the initial state left={q_value_left:.2f} nothing={q_value_nothing:.2f} right={q_value_right:.2f}\")\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Compute the action taken in the initilal state according to q-values\n",
        "# when following a greedy strategy\n",
        "\n",
        "\n",
        "## END of your code here\n",
        "\n",
        "print(f\"Action taken by the greedy policy in the initial state: {action_str[action]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov0UwoBzcaDv"
      },
      "source": [
        "The q-value of the initial state corresponds to how much (discounted) reward the agent expects to get in this episode.\n",
        "\n",
        "We will compare the estimated q-value to the discounted return of the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhhF-GJccVne"
      },
      "outputs": [],
      "source": [
        "initial_q_value = q_values.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JueeSE1xcQpK"
      },
      "source": [
        "## Step until the end of the episode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_OYobAab8SF"
      },
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "done = False\n",
        "i = 0\n",
        "\n",
        "while not done:\n",
        "    i += 1\n",
        "\n",
        "    # Display current state\n",
        "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Retrieve q-value\n",
        "    q_values = get_q_values(dqn_model, obs)\n",
        "\n",
        "    # Take greedy-action\n",
        "    action, _ = dqn_model.predict(obs, deterministic=True)\n",
        "\n",
        "    print(f\"Q-value of the current state left={q_values[0]:.2f} nothing={q_values[1]:.2f} right={q_values[2]:.2f}\")\n",
        "    print(f\"Action: {action_str[action]}\")\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    episode_rewards.append(reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBEomET1wkjN"
      },
      "source": [
        "### Exercise (3 minutes): compare estimated initial q-value with actual discounted return\n",
        "\n",
        "Compute the discounted return (sum of discounted reward) of the episode and compare it to the initial estimated q-value.\n",
        "\n",
        "Note: You will need to use the discount factor `dqn_model.gamma`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om4NNW2VdnM9"
      },
      "outputs": [],
      "source": [
        "sum_discounted_rewards = 0\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Compute the sum of discounted reward for the last episode\n",
        "# using `episode_rewards` list and `dqn_model.gamma` discount factor\n",
        "\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(f\"Sum discounted rewards: {sum_discounted_rewards:.2f}, initial q-value {initial_q_value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZloxyPKPg9HX"
      },
      "source": [
        "## Exercise (30 minutes): Double DQN\n",
        "\n",
        "In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation (the action which q-value is over-estimated will be chosen more often and this slow down training).\n",
        "\n",
        "To reduce over-estimation, double q-learning (and then double DQN) was proposed. It decouples the action selection from the value estimation.\n",
        "\n",
        "Concretely, in DQN, the target q-value is defined as:\n",
        "\n",
        "$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "where the target network `q_net_target` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n",
        "\n",
        "$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "Double DQN uses the online network `q_net` with parameters $\\mathbb{\\theta}_{online}$ to select the action and the target network `q_net_target` to estimate the associated q-values:\n",
        "\n",
        "$$Y^{DoubleDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "\n",
        "The goal in this exercise is for you to write the update method for `DoubleDQN`.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "1. Sample replay buffer data using `self.replay_buffer.sample(batch_size)`\n",
        "\n",
        "2. Compute the Double DQN target q-value using the next observations `replay_data.next_observation`, the online network `self.q_net`, the target network `self.q_net_target`, the rewards `replay_data.rewards` and the termination signals `replay_data.dones`. Be careful with the shape of each object ;)\n",
        "\n",
        "3. Compute the current q-value estimates using the online network `self.q_net`, the current observations `replay_data.observations` and the buffer actions `replay_data.actions`\n",
        "\n",
        "4. Compute the loss to train the q-network using L2 or Huber loss (`F.smooth_l1_loss`)\n",
        "\n",
        "\n",
        "Link: https://paperswithcode.com/method/double-q-learning\n",
        "\n",
        "Paper: https://arxiv.org/abs/1509.06461\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4227ILqjg8b4"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class DoubleDQN(DQN):\n",
        "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "        # Update learning rate according to schedule\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "\n",
        "        losses = []\n",
        "        for _ in range(gradient_steps):\n",
        "            ### YOUR CODE HERE\n",
        "            # Sample replay buffer\n",
        "            replay_data = ...\n",
        "\n",
        "            # Do not backpropagate gradient to the target network\n",
        "            with th.no_grad():\n",
        "                # Compute the next Q-values using the target network\n",
        "                next_q_values = ...\n",
        "                # Decouple action selection from value estimation\n",
        "                # Compute q-values for the next observation using the online q net\n",
        "                next_q_values_online = ...\n",
        "                # Select action with online network\n",
        "                next_actions_online = ...\n",
        "                # Estimate the q-values for the selected actions using target q network\n",
        "                next_q_values = ...\n",
        "\n",
        "                # 1-step TD target\n",
        "                target_q_values = ...\n",
        "\n",
        "            # Get current Q-values estimates\n",
        "            current_q_values = ...\n",
        "\n",
        "            # Retrieve the q-values for the actions from the replay buffer\n",
        "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
        "\n",
        "            # Check the shape\n",
        "            assert current_q_values.shape == target_q_values.shape\n",
        "\n",
        "            # Compute loss (L2 or Huber loss)\n",
        "            loss = ...\n",
        "\n",
        "            ### END OF YOUR CODE\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Optimize the q-network\n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Clip gradient norm\n",
        "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        # Increase update counter\n",
        "        self._n_updates += gradient_steps\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/loss\", np.mean(losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3fpWWOg_AS"
      },
      "source": [
        "## Monitoring Evolution of the Estimated q-value\n",
        "\n",
        "\n",
        "Here we create a SB3 callback to over-estimate initial q-values and then monitor their evolution over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLbQ9RhUpMOl"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "\n",
        "class MonitorQValueCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback to monitor the evolution of the q-value\n",
        "    for the initial state.\n",
        "    It allows to artificially over-estimate a q-value for initial states.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_interval: int = 2500):\n",
        "        super().__init__()\n",
        "        self.timesteps = []\n",
        "        self.max_q_values = []\n",
        "        self.sample_interval = sample_interval\n",
        "        n_samples = 512\n",
        "        env = gym.make(\"MountainCar-v0\")\n",
        "        # Sample initial states that will be used to monitor the estimated q-value\n",
        "        self.start_obs = np.array([env.reset() for _ in range(n_samples)])\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        # Create overestimation\n",
        "        obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
        "        # Over-estimate going left q-value for the initial states\n",
        "        target_q_values = th.ones((len(obs), 1), device=self.model.device).float() * 100\n",
        "\n",
        "        for _ in range(100):\n",
        "            # Get current Q-values estimates\n",
        "            current_q_values = self.model.q_net(obs)\n",
        "\n",
        "            # Over-estimate going left\n",
        "            current_q_values = th.gather(current_q_values, dim=1, index=th.zeros((len(obs), 1), device=self.model.device).long())\n",
        "\n",
        "            loss = F.mse_loss(current_q_values, target_q_values)\n",
        "\n",
        "            # Optimize the policy\n",
        "            self.model.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.model.policy.optimizer.step()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Sample q-values\n",
        "        if self.n_calls % self.sample_interval == 0:\n",
        "            # Monitor estimated q-values using current model\n",
        "            obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
        "            with th.no_grad():\n",
        "                q_values = self.model.q_net(obs).cpu().numpy()\n",
        "\n",
        "            self.logger.record(\"train/max_q_value\", float(q_values.max()))\n",
        "            self.timesteps.append(self.num_timesteps)\n",
        "            self.max_q_values.append(q_values.max())\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36dtW1c4xQUG"
      },
      "source": [
        "## Evolution of the q-value with initial over-estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-XTmT6SxdOa"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIo1EDx2xcwZ"
      },
      "outputs": [],
      "source": [
        "dqn_model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    \"MountainCar-v0\",\n",
        "    verbose=1,\n",
        "    train_freq=16,\n",
        "    gradient_steps=8,\n",
        "    gamma=0.99,\n",
        "    exploration_fraction=0.2,\n",
        "    exploration_final_eps=0.07,\n",
        "    target_update_interval=5000,\n",
        "    learning_starts=1000,\n",
        "    buffer_size=25000,\n",
        "    batch_size=128,\n",
        "    learning_rate=4e-3,\n",
        "    policy_kwargs=dict(net_arch=[256, 256]),\n",
        "    tensorboard_log=tensorboard_log,\n",
        "    seed=102,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd62HoWsxfBJ"
      },
      "source": [
        "Define the callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcULdR48xhH5"
      },
      "outputs": [],
      "source": [
        "monitor_dqn_value_cb = MonitorQValueCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z_cFIapxhR7"
      },
      "outputs": [],
      "source": [
        "dqn_model.learn(total_timesteps=int(4e4), callback=monitor_dqn_value_cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxAGOezBx3GS"
      },
      "source": [
        "### Double DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYNcgKsSegj0"
      },
      "outputs": [],
      "source": [
        "double_q = DoubleDQN(\"MlpPolicy\",\n",
        "            \"MountainCar-v0\",\n",
        "            verbose=1,\n",
        "            train_freq=16,\n",
        "            gradient_steps=8,\n",
        "            gamma=0.99,\n",
        "            exploration_fraction=0.2,\n",
        "            exploration_final_eps=0.07,\n",
        "            target_update_interval=5000,\n",
        "            learning_starts=1000,\n",
        "            buffer_size=25000,\n",
        "            batch_size=128,\n",
        "            learning_rate=4e-3,\n",
        "            policy_kwargs=dict(net_arch=[256, 256]),\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            seed=102)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWhQTCXQyBJZ"
      },
      "outputs": [],
      "source": [
        "monitor_double_q_value_cb = MonitorQValueCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvveywQUkEpW"
      },
      "outputs": [],
      "source": [
        "double_q.learn(int(4e4), log_interval=10, callback=monitor_double_q_value_cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPqcCnbnyIR5"
      },
      "source": [
        "### Evolution of the max q-value for start states over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNeKzUoaa6_K"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 3), dpi=200)\n",
        "plt.title(\"Evolution of max q-value for start states over time\")\n",
        "plt.plot(monitor_dqn_value_cb.timesteps, monitor_dqn_value_cb.max_q_values, label=\"DQN\")\n",
        "plt.plot(monitor_double_q_value_cb.timesteps, monitor_double_q_value_cb.max_q_values, label=\"Double DQN\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqRixViAv6gT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xVm9QPNVwKXN"
      ],
      "name": "dqn_sb3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sd3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
